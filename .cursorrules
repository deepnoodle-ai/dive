You are an expert principal software engineer specializing in Go, Python, AI
agents, and actor models.

## Agents Overview

This repository implements an AI agent framework in Go. Humans may ask questions
of the agents and assign them tasks. Multiple agents may form a team and
collaborate on tasks. Manager agents may assign work to other agents.

Each agent has its own run loop, which allows it to independently react to
events and make progress on tasks. Inspiration is drawn from the "Actor Model"
of concurrency, where interactions with the agents is accomplished via messages.
This message passing is hidden by the `Agent` Go interface. The "Chat", "Work",
and "Event" methods are used to pass messages to the agent.

## Large Language Models (LLMs)

This project aims to support all the leaading LLM providers, including OpenAI,
Anthropic, and Groq. The `llm` package aims to define a unified interface to
the LLM providers, while the `providers/<provider>` packages provide the
implementation for each provider. To the extent possible, tool use should be
supported by each LLM provider.

## Tool Use

Tools allow agents to perform actions. The Tool interface is as follows:

```go
type Tool interface {
	Definition() *ToolDefinition
	Call(ctx context.Context, input string) (string, error)
}
```

A simple tool definition looks like this:

```go
&llm.ToolDefinition{
    Name:        "GoogleSearch",
    Description: "Searches Google for the given query",
    Parameters: llm.Schema{
        Type:     "object",
        Required: []string{"query"},
        Properties: map[string]*llm.SchemaProperty{
            "query": {
                Type:        "string",
                Description: "The search query",
            },
            "limit": {
                Type:        "number",
                Description: "Max number of results to return (Default: 10)",
            },
        },
    },
}
```

## Folder Structure

- `/`: the agents package defines the Agent Go interface and a Team type
- `llm`: defines the interface for LLMs and supporting message types
- `prompt`: used to construct prompts for LLMs
- `providers/<provider>`: providers implement the `llm.LLM` interface
- `tools`: tools for use by LLMs

## Code Style and Structure

- Use dependency injection techniques to support testing and configuration
- The "functional options" pattern should be used in situations where many
  optional parameters are supported by a function.

## Project Code Examples

Generate a response from an LLM:

```go
response, err := agent.llm.Generate(ctx,
    p.Messages,
    llm.WithSystemPrompt("the system prompt"),
    llm.WithCacheControl("ephemeral"),
    llm.WithTools(a.getTools()...),
)
```

Run an agent:

```go
a := agents.NewStandardAgent(agents.StandardAgentSpec{
    Name:         "example-agent",
    Role:         &agents.Role{Name: "Research Assistant"},
    LLM:          anthropic.New(),
    CacheControl: "ephemeral",
    Tools:        []llm.Tool{tools.NewGoogleSearch(googleClient)},
})
if err := a.Start(ctx); err != nil {
    log.Fatal(err)
}
defer a.Stop(ctx)
```

Assign a task to an agent:

```go
task := agents.NewTask(agents.TaskSpec{Description: "Research cloud security"})
promise, err := a.Work(ctx, task)
if err != nil {
    log.Fatal(err)
}
result, err := promise.Get(ctx)
if err != nil {
    log.Fatal(err)
}
```

Build a list of messages to pass to an LLM:

```go
messages := []*llm.Message{
    llm.NewUserMessage("What is the capital of France?"),
}
```

## Testing

Use the `testify/require` package for assertions. For example:

```go
func TestHelloWorld(t *testing.T) {
	ctx := context.Background()
	provider := New()
	response, err := provider.Generate(ctx, []*llm.Message{
		llm.NewUserMessage("respond with \"hello\""),
	})
	require.NoError(t, err)
	require.Equal(t, "hello", response.Message().Text())
}
```
